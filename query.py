import os
from langchain_community.chat_models import ChatOllama
from langchain.prompts import ChatPromptTemplate, PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain.retrievers.multi_query import MultiQueryRetriever
from get_vector_db import get_vector_db

LLM_MODEL = os.getenv('LLM_MODEL', 'mistral')

# Function to get the prompt templates for generating alternative questions and answering based on context
def get_prompt():
    QUERY_PROMPT = PromptTemplate(
        input_variables=["question"],
        template="""Eres un asistente virtual de la Universidad Nacional de Hurlingham (UNAHUR). Tu tarea consiste en ayudar a los estudiantes a resolver dudas sobre materias, cursos, inscripciones, becas y trámites universitarios.
        Cuando recibas la pregunta de un usuario, genera cinco versiones alternativas de la misma pregunta. Esto ayudará a recuperar documentos relevantes de la base de datos de la universidad. Cada versión debe ser clara, concisa y académicamente comprensible.
        Proporciona las cinco preguntas alternativas separadas por saltos de línea.
        Pregunta original:{question}""",
    )

    template = """Responda la pregunta basándose ÚNICAMENTE en el siguiente contexto
    {context}
    Question: {question}
    """

    prompt = ChatPromptTemplate.from_template(template)

    return QUERY_PROMPT, prompt

# Main function to handle the query process
def query(input):
    if input:
        # Initialize the language model with the specified model name
        llm = ChatOllama(model=LLM_MODEL)
        # Get the vector database instance
        db = get_vector_db()
        # Get the prompt templates
        QUERY_PROMPT, prompt = get_prompt()

        # Set up the retriever to generate multiple queries using the language model and the query prompt
        retriever = MultiQueryRetriever.from_llm(
            db.as_retriever(), 
            llm,
            prompt=QUERY_PROMPT
        )

        # Define the processing chain to retrieve context, generate the answer, and parse the output
        chain = (
            {"context": retriever, "question": RunnablePassthrough()}
            | prompt
            | llm
            | StrOutputParser()
        )

        response = chain.invoke(input)
        
        return response

    return None